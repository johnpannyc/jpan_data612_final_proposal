{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student: Jun Pan\n",
    "    \n",
    "The requirements for the Final Project Planning Document (Proposal) are included with the requirements for the Final Project, below.\n",
    "\n",
    "Please use this submission link only for the Final Project Planning Document (proposal), due Thursday.\n",
    "\n",
    "The goal for your final project is for you to build out a recommender system using a large dataset (ex: 1M+ ratings or 10k+ users, 10k+ items. There are three deliverables, with separate dates:\n",
    "\n",
    "[1] Planning Document Find an interesting dataset and describe the system you plan to build out. If you would like to use one of the datasets you have already worked with, you should add a unique element or incorporate additional data. (i.e. explicit features you scrape from another source, like image analysis on movie posters). The overall goal, however, will be to produce quality recommendations by extracting insights from a large dataset. You may do so using Spark, or another distributed computing method, OR by effectively applying one of the more advanced mathematical techniques we have covered. There is no preference for one over the other, as long as your recommender works! The planning document should be written up and published as a notebook on GitHub or in RPubs.Please submit the link in the Unit 4 folder, due Thursday, July 5.\n",
    "\n",
    "[2] Presentation. Make a five-minute presentation of your system in our final meetup on Tuesday. If you’re not able to attend the meetup, you’re responsible for either recording your presentation, or scheduling one-on-one time to deliver your presentation prior to the meetup. You should be prepared to present on Tuesday. You should use this project to showcase some of the concepts that you have learned in this course, while delivering on the (probably) less familiar Spark platform. You are welcome to submit a compelling alternative proposal (subject to approval), such as implementing a recommender system using in Microsoft Azure ML Studio or with Google TensorFlow, or building out an application of a certain complexity using another tool. You may work in a small group (2-3) on this assignment.\n",
    "\n",
    "[2] Implementation. In this final project deliverable, you’ll build out the system that you describe in your planning document. This will be due on Thursday and must be turned in as an RMarkdown file or a Jupyter notebook, and posted to GitHub or RPubs.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposal of Final Project of DATA 612 by Jun Pan\n",
    "Build an ALS Algorithm based movie recommendation stystem Using Spark\n",
    "\n",
    "Rational: \n",
    "MovieLens is a web-based recommender system and virtual community that recommends movies for its users to watch, based on their film preferences using collaborative filtering of members' movie ratings and movie reviews. It contains about 11 million ratings for about 8500 movies.[1] MovieLens was created in 1997 by GroupLens Research, a research lab in the Department of Computer Science and Engineering at the University of Minnesota in order to gather research data on personalized recommendations. Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. 'spark.mllib' currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. 'spark.mllib' uses the alternating least squares (ALS) algorithm to learn these latent factors [2].  When using collaborative filtering, getting recommendations is not as simple as predicting for the new entries using a previously generated model. Instead, we need to train again the model but including the new user preferences in order to compare them with other users in the dataset. That is, the recommender needs to be trained every time we have new user ratings (although a single model can be used by multiple users). This makes the process expensive, and it is one of the reasons why scalability is a problem (and Spark a solution!). Once we have our model trained, we can reuse it to obtain top recomendations for a given user or an individual rating for a particular movie. These are less costly operations than training the model itself. Therefore, Apache Spark is a lightning-fast cluster computing designed for fast computation on big datasets. It was built on top of Hadoop MapReduce and it extends the MapReduce model to efficiently use more types of computations which includes Interactive Queries and Stream Processing [3]. Previously, we have built user-user, item-item collaborative filtering, SVD, matrix factorized recommendation systems. We have not tried ALS recommendation system.  Also, we have experience computational difficulty due to large datasets for user-user, and item-item collaborative filtering system. \n",
    "we only used 1% of data to build the recommendation system  In order to speed up the computation and not sacrifized valuable information.  We are going to introduce Apach Spark in this project.\n",
    "\n",
    "Method: \n",
    "Movielens datasets will get from the grouplens website (https://grouplens.org/datasets/movielens/). \n",
    "In this study, MovieLens dataset (1M) will be used to build a movie recommender using collaborative filtering with Spark's Alternating Least Saqures implementation. Firstly, we are going to get and parse movies and ratings data into Spark RDDs. Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes [4]. Secondly, we are going to build an ALS based recommender system. ALS recommender is a matrix factorization algorithm that uses Alternating Least Squares with Weighted-Lamda-Regularization (ALS-WR). It makes the regularization parameter less dependent on the scale of the dataset. This way the best parameter learned from the sampled subset can be aplied to the full dataset and we will get similar performance. The latent factors should explain the observed user to item ratings and map new users to optimal movie recommendations.  Accuracy of ALS recommendation system will be checked by using RMSE.\n",
    "\n",
    "\n",
    "Question to be addressed:\n",
    "1. Getting top recommendation by movie IDs;\n",
    "2. Predicted rating for a particular movie for a given user;\n",
    "3. Top rated movie posters will be displayed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. http://license.umn.edu/technologies/z05173_movielens-database\n",
    "2. https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html\n",
    "3. https://www.tutorialspoint.com/apache_spark/\n",
    "4. https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
